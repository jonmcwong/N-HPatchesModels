{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nadam_Baseline_20e_Denoise.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonmcwong/N-HPatchesModels/blob/master/DenoiseNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "scylC1qNuBY0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Baseline Code\n",
        "\n",
        "This code introduces a two-step training for the N-HPatches problem. In N-HPatches problem, we aim to generate a patch descriptor that is able to perform successfully tasks such as matching, retrieval or verification. \n",
        "\n",
        "Contrary to classical HPatches dataset, in N-HPatches, images contain random non-smooth perturbations produced by a synthetic noise. This noise could be critical when training the descriptor, therefore, we introduce a denoising model that could help us to deal with those perturbations. Denoising models have been already introduced in the course [tutorials](https://github.com/MatchLab-Imperial/deep-learning-course) and lectures, their objective is to generate a clean/denoised version of the input image.  We will refer in this code to the images with noise as `noisy`, to the images after applying the denoise model as `denoised` and the original patches from HPatches (so no extra noise added) which are used as ground-truth for the denoising step as `clean`. \n",
        "\n",
        "\n",
        "Thus, we aim to minimize the noise in images before the second step, which is computing a feature vector, also called descriptor. Those descriptions must be a powerful representation of the input patches. The idea behind is that if two descriptors belong two similar patches, they should be close to each other, i.e. have a low Euclidean distance. See figure below:\n",
        "\n",
        "![](https://i.ibb.co/4tvm3Vh/descriptorspace.png)\n",
        "\n",
        "This baseline code gives a method you can use to compare to whatever another approach you develop.  There are several other approaches you can test to see if there is any improvement, e.g. train the descriptor directly with noisy patches, without the denoising model. However, this code provides some guidance about how to implement the different blocks, how to stack them if desired, how to read the data and how to evaluate the method.\n",
        "\n",
        "The values given can be improved without changing the core method, only by tuning correctly the hyperparameters or giving it more training time, among others.\n",
        "\n",
        "As a first step of the project, you should get familiar with the problem and the provided code, so you can develop more complex and robust algorithms afterward. "
      ]
    },
    {
      "metadata": {
        "id": "iamuRgeiNLjW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Safety Check\n",
        "\n",
        "As Google Colab is an external platform, we cannot guarantee that everytime you connect to a remote server, you will have the same amount of RAM or video RAM. For that reason, we will first check the amount of memory we have in the notebook. RAM should be around 12.9 GB, which is enough to load the datasets in memory. Also, usually, we have available 11.4 GB of GPU memory, which is more than enough to run this code. However, some users reported having only 500 MB of GPU memory. If you have that amount, restart the environment to see if you get the corresponding 11.4 GB."
      ]
    },
    {
      "metadata": {
        "id": "ZZG4BqkENEyd",
        "colab_type": "code",
        "outputId": "eeebd9b3-1bdf-4135-bf18-a7b30f65921a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "cell_type": "code",
      "source": [
        "# Taken from\n",
        "# https://stackoverflow.com/questions/48750199/google-colaboratory-misleading-information-about-its-gpu-only-5-ram-available\n",
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# Colab only provides one GPU and it is not always guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python2.7/dist-packages (0.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BBvIvBoyg68g",
        "colab_type": "code",
        "outputId": "e882b79d-8972-4c25-f5d5-de545feab179",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "printm()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('RAM Free: 12.9 GB', ' | Proc size: 151.8 MB')\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r92xOupMj6o_",
        "colab_type": "code",
        "outputId": "43c7e332-7d5f-4c07-9b8c-7572c44b418b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "#set working directory in google drive so that we can import models and history\n",
        "path = '/content/drive/My\\ Drive/Imperial\\ Year\\ 3/Study/Spring/DeepLearning/Baseline_Nadam_20e'\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OMiynJ7p-zI8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Downloading Functions and Data\n",
        "\n",
        "The first step is to clone the GitHub repository of the course, which contains already implemented functions. You can use your own function and import them here doing the same. In addition, we are going to download and extract the N-HPatches data. \n",
        "\n",
        "As a note, in colab, we can run terminal commands by using ```!```. Also, by using ```%``` we have access to the [built-in IPython magic commands](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-cd), which we will use to move through directories (`cd` command). It takes around 5 minutes to download and unzip the dataset. \n"
      ]
    },
    {
      "metadata": {
        "id": "yV1m-9ZGuKGj",
        "colab_type": "code",
        "outputId": "828c0259-392a-4a32-efb6-27e2cc4df262",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "cell_type": "code",
      "source": [
        "!cd /content\n",
        "\n",
        "# Clone repo\n",
        "!git clone https://github.com/MatchLab-Imperial/keras_triplet_descriptor"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'keras_triplet_descriptor'...\n",
            "remote: Enumerating objects: 50, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 178 (delta 25), reused 22 (delta 10), pack-reused 128\u001b[K\n",
            "Receiving objects: 100% (178/178), 149.84 MiB | 19.34 MiB/s, done.\n",
            "Resolving deltas: 100% (58/58), done.\n",
            "Checking out files: 100% (69/69), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pyZSqhZ5LACT",
        "colab_type": "code",
        "outputId": "2ef16a86-838e-43e3-c358-034afc9eaf69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Change directory\n",
        "%cd /content/keras_triplet_descriptor    \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/keras_triplet_descriptor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "307CBCL-FjX4",
        "colab_type": "code",
        "outputId": "157a582e-ebbe-4993-9d45-4a2b4e61a3a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "cell_type": "code",
      "source": [
        "# Download data\n",
        "!wget -O hpatches_data.zip https://imperialcollegelondon.box.com/shared/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-06 14:39:53--  https://imperialcollegelondon.box.com/shared/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip\n",
            "Resolving imperialcollegelondon.box.com (imperialcollegelondon.box.com)... 185.235.236.197\n",
            "Connecting to imperialcollegelondon.box.com (imperialcollegelondon.box.com)|185.235.236.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /public/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip [following]\n",
            "--2019-03-06 14:39:53--  https://imperialcollegelondon.box.com/public/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip\n",
            "Reusing existing connection to imperialcollegelondon.box.com:443.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://imperialcollegelondon.app.box.com/public/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip [following]\n",
            "--2019-03-06 14:39:53--  https://imperialcollegelondon.app.box.com/public/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip\n",
            "Resolving imperialcollegelondon.app.box.com (imperialcollegelondon.app.box.com)... 185.235.236.199\n",
            "Connecting to imperialcollegelondon.app.box.com (imperialcollegelondon.app.box.com)|185.235.236.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://public.boxcloud.com/d/1/b1!P-mBCjsrrrprY6ZaiK-N-EZdqrhSwBXoS--Y_WMIUVFpRvBW15lPn3d0zCJ1kGxehQ0PXm4exL7Z-UvIxh7yjT83QPH8kiRvWa9GL0M5Q23lxRRd2wShsor7H5V6wAr1yUFpseBzHHcZKJF3Gtw9I-23WgA2T-UMkI0X8Wx3z8ugxGffjLDHAv88KI7BfqLLyLeqCsKv2bndHcgtE9cxcsunw2KlCYx9BuKsTs6fG5A-mD0sCxqEHldkCHk0YCeyYHJDCgrhXiZchHI33ksz5j14aH-VtSgvxoR6k2eczC3gopsvPe60nvsW2pXOaUQ0ZAGwdk4bx8fJ3kEwFhFGL-bwobn_Vg5ZrtBpWX3BGmtHNFMMN98W4px6IKLLTBFCQ3JCUj1Ej1XAFFdKRvalXHjsxDfp8AoigM_JaPQ_1LNtVXDVEpJ4Uihxsa5tmR8JFGUZhvtIyz5McSGIJSnpBZWyq_xkaq4gzaPmpIxDXzoHUPkgY_ASErUdiCYeqQrRQVft7ArJiuYyDJV0bvb4DvoGvdM3-3CaZo_HLJztiPLtfnCdTshFBpfja0fbP1rXlnySWfrnY4HtFkJk0O13bKukbnh7VGbWZA-U1z0Ej_a2fRpScuuWZf8JrAPM78H3G0M4aoQY8j7dvOYekXidQMol0gkkZZ53E37vV6okcnw-qoxIgcijJbfOhrWv4qxhD5POP7KMHsov_RNhdMBi6BJcMLnJcWGc4i7I0wo8-LfH8KBAQL_JtuKQtmyoOBmaH-dnfUJNT0JfGLGWE0l0MkGA8pM1Cq1bmqykgNedOPAX-8Q_UCZN60bqZCY8C2_OvMFOxHodobeKfY2i9tVPXYBMdT9IctPKTOG_tIwcit3rbkHIcgYbH6nOoXuwbzA0G-Mjfhx9SENjEdcgvVVx3fImD3dP92gsUHYHZBJHcr5KtYBvtXRD1yfPyASFtLR5iR7IuB6WIvc9H0Q5k2Q20xgc9oZB-C3-kdqzvNioQ5WvY45HHwIeRvfvwmFutyvhxEfPu5vkcGgd_i51T_LAcyPmgsVI3LkPNYlBX1GIh4c0IzOR0xfvqDJ4RCSAb6LcJrgYu1oj0GQB1ImTo283H06bL4D0OEMCFRG_CLXxhVW7JoH2ZMKM4zhXedq1043ULW8U8paJi21mC5ME_c_jw2Kyb8UE3dAs_9f0hwwSmeoXj8eFjspsDbAgp_hYOJmF0BOMAkna3SZTiht_mmi7VzMS1-RiZXULQYoK4Ph8DI465yJWZhH0acEhkaZ8sXgQpsIEoM5E4kqbdQFjHTmCcChaiYL36QGYeWCveKU7co8PMYH7bwG_Hgvxm3vpCSU9nmH_1YIu7jTk9YuswDh4enXF27fdXFnG-yGLdPwnZp4llSEVYQZprZT1Ahv6NcCvYWUmT8ROhbozMfE_jzrzT1UU3BK5nURqwqxtYD5OUFcTMSxVFXuBi0mia-vUUZQoBMzc69s_KfUu9Ua1BUiG-ERBo0Rj4Np81SuX4w../download [following]\n",
            "--2019-03-06 14:39:54--  https://public.boxcloud.com/d/1/b1!P-mBCjsrrrprY6ZaiK-N-EZdqrhSwBXoS--Y_WMIUVFpRvBW15lPn3d0zCJ1kGxehQ0PXm4exL7Z-UvIxh7yjT83QPH8kiRvWa9GL0M5Q23lxRRd2wShsor7H5V6wAr1yUFpseBzHHcZKJF3Gtw9I-23WgA2T-UMkI0X8Wx3z8ugxGffjLDHAv88KI7BfqLLyLeqCsKv2bndHcgtE9cxcsunw2KlCYx9BuKsTs6fG5A-mD0sCxqEHldkCHk0YCeyYHJDCgrhXiZchHI33ksz5j14aH-VtSgvxoR6k2eczC3gopsvPe60nvsW2pXOaUQ0ZAGwdk4bx8fJ3kEwFhFGL-bwobn_Vg5ZrtBpWX3BGmtHNFMMN98W4px6IKLLTBFCQ3JCUj1Ej1XAFFdKRvalXHjsxDfp8AoigM_JaPQ_1LNtVXDVEpJ4Uihxsa5tmR8JFGUZhvtIyz5McSGIJSnpBZWyq_xkaq4gzaPmpIxDXzoHUPkgY_ASErUdiCYeqQrRQVft7ArJiuYyDJV0bvb4DvoGvdM3-3CaZo_HLJztiPLtfnCdTshFBpfja0fbP1rXlnySWfrnY4HtFkJk0O13bKukbnh7VGbWZA-U1z0Ej_a2fRpScuuWZf8JrAPM78H3G0M4aoQY8j7dvOYekXidQMol0gkkZZ53E37vV6okcnw-qoxIgcijJbfOhrWv4qxhD5POP7KMHsov_RNhdMBi6BJcMLnJcWGc4i7I0wo8-LfH8KBAQL_JtuKQtmyoOBmaH-dnfUJNT0JfGLGWE0l0MkGA8pM1Cq1bmqykgNedOPAX-8Q_UCZN60bqZCY8C2_OvMFOxHodobeKfY2i9tVPXYBMdT9IctPKTOG_tIwcit3rbkHIcgYbH6nOoXuwbzA0G-Mjfhx9SENjEdcgvVVx3fImD3dP92gsUHYHZBJHcr5KtYBvtXRD1yfPyASFtLR5iR7IuB6WIvc9H0Q5k2Q20xgc9oZB-C3-kdqzvNioQ5WvY45HHwIeRvfvwmFutyvhxEfPu5vkcGgd_i51T_LAcyPmgsVI3LkPNYlBX1GIh4c0IzOR0xfvqDJ4RCSAb6LcJrgYu1oj0GQB1ImTo283H06bL4D0OEMCFRG_CLXxhVW7JoH2ZMKM4zhXedq1043ULW8U8paJi21mC5ME_c_jw2Kyb8UE3dAs_9f0hwwSmeoXj8eFjspsDbAgp_hYOJmF0BOMAkna3SZTiht_mmi7VzMS1-RiZXULQYoK4Ph8DI465yJWZhH0acEhkaZ8sXgQpsIEoM5E4kqbdQFjHTmCcChaiYL36QGYeWCveKU7co8PMYH7bwG_Hgvxm3vpCSU9nmH_1YIu7jTk9YuswDh4enXF27fdXFnG-yGLdPwnZp4llSEVYQZprZT1Ahv6NcCvYWUmT8ROhbozMfE_jzrzT1UU3BK5nURqwqxtYD5OUFcTMSxVFXuBi0mia-vUUZQoBMzc69s_KfUu9Ua1BUiG-ERBo0Rj4Np81SuX4w../download\n",
            "Resolving public.boxcloud.com (public.boxcloud.com)... 185.235.236.200\n",
            "Connecting to public.boxcloud.com (public.boxcloud.com)|185.235.236.200|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4088106554 (3.8G) [application/zip]\n",
            "Saving to: ‘hpatches_data.zip’\n",
            "\n",
            "hpatches_data.zip   100%[===================>]   3.81G  21.0MB/s    in 3m 5s   \n",
            "\n",
            "2019-03-06 14:42:59 (21.0 MB/s) - ‘hpatches_data.zip’ saved [4088106554/4088106554]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "36mBTFvPCxY9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Extract data\n",
        "!unzip -q ./hpatches_data.zip\n",
        "!rm ./hpatches_data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rjyr96hR_4wS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Importing Necessary Modules\n",
        "\n",
        "We now import the modules we will use in this baseline code. "
      ]
    },
    {
      "metadata": {
        "id": "o0KYfe-at9KN",
        "colab_type": "code",
        "outputId": "6b4f6123-7f83-4da9-f767-771e8455ae16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda, Reshape\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization \n",
        "from keras.layers import Input, UpSampling2D, concatenate  \n",
        "\n",
        "from read_data import HPatches, DataGeneratorDesc, hpatches_sequence_folder, DenoiseHPatches, tps\n",
        "from utils import generate_desc_csv, plot_denoise, plot_triplet"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "AFG0LyAct_-l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The `read_data` and `utils` imports are functions provided in the repository we just cloned. You can navigate through the *File tab* and check what those functions do for a better understanding.\n",
        "\n",
        "![texto del enlace](https://i.ibb.co/HnfSvfT/filetab.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "2Y61ZKWZ7o5k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We also fix the seeds of the pseudo-random number generators to have reproducible results. The idea of fixing the seed is having the same results every time the algorithm is run if there are no changes in the code."
      ]
    },
    {
      "metadata": {
        "id": "NXL31ez-AT5h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "random.seed(1234)\n",
        "np.random.seed(1234)\n",
        "tf.set_random_seed(1234)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_OqFkNujBGzf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we load the data. The original HPatches dataset has several splits, which are used to separate the available sequences in train sequences and test sequences. For our experiments in N-HPatches we use the same splits as in HPatches. Specifically, we load (and report results) using the split `'a'`:\n"
      ]
    },
    {
      "metadata": {
        "id": "ABKDHB9RApZk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hpatches_dir = './hpatches'\n",
        "splits_path = './splits.json'\n",
        "\n",
        "splits_json = json.load(open(splits_path, 'rb'))\n",
        "split = splits_json['a']\n",
        "\n",
        "train_fnames = split['train']\n",
        "test_fnames = split['test']\n",
        "\n",
        "seqs = glob.glob(hpatches_dir+'/*')\n",
        "seqs = [os.path.abspath(p) for p in seqs]   \n",
        "seqs_train = list(filter(lambda x: x.split('/')[-1] in train_fnames, seqs)) \n",
        "seqs_test = list(filter(lambda x: x.split('/')[-1] in split['test'], seqs)) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qeWik0vMEtuC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Models and loss"
      ]
    },
    {
      "metadata": {
        "id": "LYJz8BDzBkIx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now define three functions that define the main modules of our baseline. \n",
        "\n",
        "*   **get_denoise_model(..)** returns the denoising model. The input for the function is the size of the patch, which will be *1x32x32*, and it outputs a keras denoising model. \n",
        "*   **get_descriptor_model(..)** builts the descriptor model. The input for the function is the size of the patch, which will be *1x32x32*, and it outputs a keras descriptor model. The model we use as baseline returns a descriptor of dimension *128x1*.\n",
        "*   **triplet_loss(..)** defines the loss function which is used to train the descriptor model. \n",
        "\n",
        "You can modify the models in these functions and run the training code again. For example, the given denoising model is quite shallow, maybe using a deeper network can improve results. Or testing new initializations for the weights. Or maybe adding dropout. Or modifying the loss function somehow..."
      ]
    },
    {
      "metadata": {
        "id": "W6QbkHnbuIUD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_denoise_model(shape):\n",
        "    \n",
        "  inputs = Input(shape)\n",
        "  \n",
        "  ## Encoder starts\n",
        "  conv1 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "  pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "  \n",
        "  ## Bottleneck\n",
        "  conv2 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "\n",
        "  ## Now the decoder starts\n",
        "  up3 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv2))\n",
        "  merge3 = concatenate([conv1,up3], axis = -1)\n",
        "  conv3 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge3)\n",
        "    \n",
        "  conv4 = Conv2D(1, 3,  padding = 'same')(conv3)\n",
        "\n",
        "  shallow_net = Model(inputs = inputs, outputs = conv4)\n",
        "  \n",
        "  return shallow_net\n",
        "\n",
        "\n",
        "\n",
        "def get_descriptor_model(shape):\n",
        "  \n",
        "  '''Architecture copies HardNet architecture'''\n",
        "  \n",
        "  init_weights = keras.initializers.he_normal()\n",
        "  \n",
        "  descriptor_model = Sequential()\n",
        "  descriptor_model.add(Conv2D(32, 3, padding='same', input_shape=shape, use_bias = True, kernel_initializer=init_weights))\n",
        "  descriptor_model.add(BatchNormalization(axis = -1))\n",
        "  descriptor_model.add(Activation('relu'))\n",
        "\n",
        "  descriptor_model.add(Conv2D(32, 3, padding='same', use_bias = True, kernel_initializer=init_weights))\n",
        "  descriptor_model.add(BatchNormalization(axis = -1))\n",
        "  descriptor_model.add(Activation('relu'))\n",
        "\n",
        "  descriptor_model.add(Conv2D(64, 3, padding='same', strides=2, use_bias = True, kernel_initializer=init_weights))\n",
        "  descriptor_model.add(BatchNormalization(axis = -1))\n",
        "  descriptor_model.add(Activation('relu'))\n",
        "\n",
        "  descriptor_model.add(Conv2D(64, 3, padding='same', use_bias = True, kernel_initializer=init_weights))\n",
        "  descriptor_model.add(BatchNormalization(axis = -1))\n",
        "  descriptor_model.add(Activation('relu'))\n",
        "\n",
        "  descriptor_model.add(Conv2D(128, 3, padding='same', strides=2,  use_bias = True, kernel_initializer=init_weights))\n",
        "  descriptor_model.add(BatchNormalization(axis = -1))\n",
        "  descriptor_model.add(Activation('relu'))\n",
        "\n",
        "  descriptor_model.add(Conv2D(128, 3, padding='same', use_bias = True, kernel_initializer=init_weights))\n",
        "  descriptor_model.add(BatchNormalization(axis = -1))\n",
        "  descriptor_model.add(Activation('relu'))\n",
        "  descriptor_model.add(Dropout(0.3))\n",
        "\n",
        "  descriptor_model.add(Conv2D(128, 8, padding='valid', use_bias = True, kernel_initializer=init_weights))\n",
        "  \n",
        "  # Final descriptor reshape\n",
        "  descriptor_model.add(Reshape((128,)))\n",
        "  \n",
        "  return descriptor_model\n",
        "  \n",
        "  \n",
        "def triplet_loss(x):\n",
        "  \n",
        "  output_dim = 128\n",
        "  a, p, n = x\n",
        "  _alpha = 1.0\n",
        "  positive_distance = K.mean(K.square(a - p), axis=-1)\n",
        "  negative_distance = K.mean(K.square(a - n), axis=-1)\n",
        "  \n",
        "  return K.expand_dims(K.maximum(0.0, positive_distance - negative_distance + _alpha), axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RlS5zcV7EJgp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Denoising Image Patches\n"
      ]
    },
    {
      "metadata": {
        "id": "wHxHwjUd3-pY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We use the *DenoiseHPatches* class implemented in the read_data.py file, which takes as input the list of sequences to load and the size of batches. \n",
        "\n",
        "*DenoiseHPatches* outputs batches where the input data is the noisy image and the label is the clean image, so we can use a mean absolute error (MAE) metric as loss function. You can try to use different metrics here to see if that improves results. \n",
        "\n",
        "Afterward, we take a subset of training and validation sequences by using *random.sample* (3 sequences for training and 1 for validation data). The purpose of doing so is just to speed-up training when trying different setups, but you should use the whole dataset when training your final model. Remove the random.sample function to give the generator all the training data.\n",
        "\n",
        "In addition, note that we are using the test set as validation. We will provide you with a new test set that will be used to evaluate your final model, and from which you will not have the clean images. \n",
        "\n",
        "**Updated**: Training should be quite faster now (1 epoch around 15 minutes)."
      ]
    },
    {
      "metadata": {
        "id": "m_VPSHmSK0dS",
        "colab_type": "code",
        "outputId": "ef2af8e2-55ca-4818-baf5-27ffc6879801",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "#denoise_generator = DenoiseHPatches(random.sample(seqs_train, 3), batch_size=50)\n",
        "#denoise_generator_val = DenoiseHPatches(random.sample(seqs_test, 1), batch_size=50)\n",
        "\n",
        "# Uncomment following lines for using all the data to train the denoising model\n",
        "denoise_generator = DenoiseHPatches(seqs_train, batch_size=64)\n",
        "denoise_generator_val = DenoiseHPatches(seqs_test, batch_size=64)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 76/76 [01:05<00:00,  1.49it/s]\n",
            "100%|██████████| 40/40 [00:39<00:00,  1.16s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ksLiLDfFiHKK",
        "colab_type": "code",
        "outputId": "0c336e01-c77e-457e-9329-a10c96a99fbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorboardcolab\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardcolab in /usr/local/lib/python2.7/dist-packages (0.0.22)\n",
            "--2019-03-06 14:45:35--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 54.174.228.92, 52.45.111.123, 52.87.35.92, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|54.174.228.92|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14809752 (14M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  14.12M  14.3MB/s    in 1.0s    \n",
            "\n",
            "2019-03-06 14:45:37 (14.3 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [14809752/14809752]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hTpxzfVYiIee",
        "colab_type": "code",
        "outputId": "16398352-cc6a-4558-b68e-ccfb72733feb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorboardcolab as tbc\n",
        "K.clear_session()\n",
        "tboard = tbc.TensorBoardColab()\n",
        "from tensorboardcolab import TensorBoardColabCallback"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "http://642d6741.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-eUSba93Dttj",
        "colab_type": "code",
        "outputId": "7a1173d3-4d12-4154-8911-e8683f192807",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "cell_type": "code",
      "source": [
        "shape = (32, 32, 1)\n",
        "denoise_model = get_denoise_model(shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "H3wkjkpk4bRh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We set number of epochs to 1, tweak it, along with other hyperparameters, to improve the performance of the model."
      ]
    },
    {
      "metadata": {
        "id": "h7J72t8tkPSj",
        "colab_type": "code",
        "outputId": "8fd9b36b-0930-4459-dbcc-dbd5fd929b73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "%cd $path\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Imperial Year 3/Study/Spring/DeepLearning/Baseline_Nadam_20e\n",
            "denoise_history.json  Graph  Nadam_Baseline_20e_Denoise.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DnIqYwGbySoK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LossHistory(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        self.losses.append(logs.get('loss'))\n",
        "    \n",
        "\n",
        "history = LossHistory()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "edwbgE6yKqcD",
        "colab_type": "code",
        "outputId": "aca4eacf-a878-42dc-aa0c-34d9a25f678e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "cell_type": "code",
      "source": [
        "# sgd = keras.optimizers.SGD(lr=0.00001, momentum=0.9, nesterov=True)\n",
        "nadam= keras.optimizers.Nadam()\n",
        "denoise_model.compile(loss='mean_absolute_error', optimizer=nadam, metrics=['mae'])\n",
        "\n",
        "### Use a loop to save for each epoch the weights in an external website in\n",
        "### case colab stops. Every time you call fit/fit_generator the weigths are NOT\n",
        "### reset, so e.g. calling 5 times fit(epochs=1) behave as fit(epochs=5)\n",
        "denoise_history = denoise_model.fit_generator(generator=denoise_generator, \n",
        "                                                epochs=30, verbose=1, \n",
        "                                                validation_data=denoise_generator_val,\n",
        "                                                callbacks=[TensorBoardColabCallback(tboard), history])\n",
        "### Saves optimizer and weights\n",
        "denoise_model.save('denoise.h5') \n",
        "### Uploads files to external hosting\n",
        "!curl -F \"file=@denoise.h5\" https://file.io\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/30\n",
            "24358/24358 [==============================] - 847s 35ms/step - loss: 5.8802 - mean_absolute_error: 5.8802 - val_loss: 5.3897 - val_mean_absolute_error: 5.3897\n",
            "Epoch 2/30\n",
            "24358/24358 [==============================] - 846s 35ms/step - loss: 5.2616 - mean_absolute_error: 5.2616 - val_loss: 5.1352 - val_mean_absolute_error: 5.1352\n",
            "Epoch 3/30\n",
            "24358/24358 [==============================] - 843s 35ms/step - loss: 5.1962 - mean_absolute_error: 5.1962 - val_loss: 5.1487 - val_mean_absolute_error: 5.1487\n",
            "Epoch 4/30\n",
            "24358/24358 [==============================] - 844s 35ms/step - loss: 5.1662 - mean_absolute_error: 5.1662 - val_loss: 5.1119 - val_mean_absolute_error: 5.1119\n",
            "Epoch 5/30\n",
            "13287/24358 [===============>..............] - ETA: 5:25 - loss: 5.1530 - mean_absolute_error: 5.1530Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2dKBM4qA8GTw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After every epoch, the code will generate an external link, this link saves your weights in case of colab disconnecting during training. Example of an epoch:\n",
        "\n",
        "**Epoch 1/1**\n",
        "1797/1797 [==============================] - 48s 27ms/step - loss: 11.4135 - \n",
        "mean_absolute_error: 11.4135 - val_loss: 7.6013 - val_mean_absolute_error: 7.6013 \n",
        "{\"success\":true,\"key\":\"fv9vjj\"\n",
        "\n",
        "\"link\":\"https://file.io/fv9vjj\",\"expiry\":\"14 days\"} **Epoch 1/1**"
      ]
    },
    {
      "metadata": {
        "id": "Ohb6Q94z4yya",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If colab did not disconnect, and you want to save the weights in your local disk, you also can use:\n"
      ]
    },
    {
      "metadata": {
        "id": "GjAQRnPV47BI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('denoise.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RKZQ8P22kqsQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "denoise_model.save('denoise_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H3kjkInroNR6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('denoise_history.json', 'w') as f:\n",
        "  json.dump(denoise_history.history,f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vCfE3xnF8Nfc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Moreover, if you have a model saved from a previous training session, you can upload it to colab and initialize the model's weights with it. \n",
        "\n",
        "You either can use `!wget download_link` or upload the weights from your local disk by using the left panel ('Files' section) in colab.\n",
        "\n",
        "Once the weights are uploaded, you can use\n",
        "\n",
        "> ``denoise_model = keras.models.load_model('./denoise.h5')\n",
        "``\n",
        "\n",
        "to load the weights."
      ]
    },
    {
      "metadata": {
        "id": "IeZGcRXDuAeG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "lmlSHyMsw9cQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "denoise_model = keras.models.load_model('./denoise_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Yusp78ww-M9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(denoise_model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}